\relax 
\catcode 95\active
\@writefile{toc}{\contentsline {chapter}{Introduction}{1}}
\@writefile{toc}{\contentsline {chapter}{Chapter 1: The Shortest Path Problem}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Graphs Preliminaries}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A diagram depicting a directed graph of the ``hyperlinks to'' relation between Wikipedia articles.}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces This figure will be the undirected graph country example in the final version.}}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Description}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Graph Representation}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}The Bellman-Ford Algorithm}{7}}
\newlabel{sec:bf}{{1.4}{7}}
\newlabel{eq:did}{{1.1}{7}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces edge-relax(edge)}}{7}}
\newlabel{edgerelax}{{1}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Dijkstra's Algorithm}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Preliminaries}{8}}
\newlabel{eoc}{{1.2}{9}}
\newlabel{pweight}{{1.3}{9}}
\newlabel{worest}{{1.4}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Algorithm Description}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}Pseudo-Code and Analysis}{10}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Dijkstra's Algorithm}}{11}}
\newlabel{dijkstrapseudo}{{2}{11}}
\@writefile{toc}{\contentsline {chapter}{Chapter 2: GPU Hardware}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Graphics Pipeline}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Overview}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The graphics pipeline.}}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}The GPU Approach}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}The Evolution of GPU Architecture}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Architecture of the Modern GPU}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Overview}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The basic architecture of the modern CUDA/OpenCL capable GPU.}}{16}}
\newlabel{gpuarchitecture}{{2.2.1}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}The Streaming Multi-Processor}{17}}
\newlabel{smpsec}{{2.2.2}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The architecture of a single building block, which contains two streaming multiprocessors.}}{17}}
\newlabel{streamingmp}{{2.2.2}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Shared Memory}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Global Memory}{18}}
\newlabel{globalmem}{{2.2.4}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}GPU Threads}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Contrasting CPU and GPU Threads}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}GPUs are SPMD}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Summary}{20}}
\@writefile{toc}{\contentsline {chapter}{Chapter 3: GPU Programming}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}GPU Configuration and Open CL}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Overview}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Open CL Kernels}{21}}
\newlabel{Kernels}{{3.1.2}{21}}
\newlabel{workgroups}{{3.1.2}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}The OpenCL Kernel Language}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An example global work group of size 16x16 with local workgroups of size 4x4.}}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}The Host Program}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Principles of GPU Programming}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}GPU Memory Word Size}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Hiding Memory Latency}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Coalescing Global Memory Access}{25}}
\newlabel{scload}{{3.2.3}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A simple coalesced load on a word size of 4 elements}}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces An access pattern that thwarts coalesced loading on devices with compute capability 1.0}}{26}}
\newlabel{nogo}{{3.2.3}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Global Memory Access Conflicts}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Local Memory Access Conflicts}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Data Transfer}{28}}
\@writefile{toc}{\contentsline {chapter}{Chapter 4: GPU Programming Examples}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Matrix Multiplication}{31}}
\newlabel{ch:mm}{{4.1}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Definition}{31}}
\newlabel{mmd}{{4.1.1}{31}}
\newlabel{entry}{{4.1}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A graphical depiction of the matrix multiplication procedure.}}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}GPU Implementation}{32}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces A Naive Matrix Multiply Kernel}}{33}}
\newlabel{naivemm}{{3}{33}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces An Improved Matrix Multiply Kernel}}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Matrix Multiplication Subdivision Identity}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Efficient Implementation}{35}}
\newlabel{mmd}{{4.1.4}{35}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Final Matrix Multiplication Kernel}}{36}}
\newlabel{alg:mmf}{{5}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A graphical representation of the bank distribution of two differently sized two dimensional arrays.}}{36}}
\newlabel{fig:bc}{{4.2}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Prefix Sums}{37}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces A sequential implementation of the prefix sum operation.}}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Parallel-Left-Fold}{38}}
\newlabel{assocident}{{4.2}{38}}
\newlabel{pfoldi}{{4.3}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces An illustration of the successive reduction of parallel-left-fold where $\oplus $ is set to be addition.}}{39}}
\newlabel{plfd}{{4.3}{39}}
\newlabel{plf}{{4.2.1}{40}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces parallel-left-fold where $p = n/2$ and $n=2^k$}}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Each box in the figure that represents the final value in a slot of the temporary array is circled. A solid circle indicates that the value is complete, while a dashed circle indicates that the value is incomplete.}}{40}}
\newlabel{nonumtree}{{4.4}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Each processor is assigned a value in the array. Every processor is active until the stride makes it so that the destination is not a valid entry in the array.}}{41}}
\newlabel{fig:naivesums}{{4.5}{41}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces A naive prefix sums algorithm}}{42}}
\newlabel{alg:naivepsums}{{8}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Work Efficient Parallel-Prefix-Sum}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Performance Analysis and Generalization}{44}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces parallel-prefix-sums where $p = n/2$ and $n=2^k$}}{45}}
\newlabel{epsums}{{9}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}An Aside}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}OpenCL Implementation}{47}}
\newlabel{sec:opclpsums}{{4.2.5}{47}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces The Prefix Sums CPU Host Program}}{49}}
\newlabel{alg:psumshost}{{10}{49}}
\@writefile{toc}{\contentsline {chapter}{Chapter 5: GPU Graph Algorithms}{51}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Modified Matrix Multiplication}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Algorithm Description}{51}}
\newlabel{eq:ajmdef}{{5.1}{51}}
\newlabel{eq:mmadid}{{5.2}{51}}
\newlabel{eq:fmmid}{{5.3}{52}}
\newlabel{yes}{{5.4}{52}}
\newlabel{alg:pbf}{{5.2}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}A Performance Optimization}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Calculating Paths}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Detecting Negative Cycles}{54}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Parallelized Bellman-Ford}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Algorithm Description}{55}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {11}{\ignorespaces A pseudo-code description of Parallelized Bellman Ford.}}{56}}
\newlabel{alg:pbf}{{11}{56}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Input Parallel Bellman-Ford}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Algorithm Description}{58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}A Possible Refinement}{59}}
\@writefile{toc}{\contentsline {chapter}{Conclusion}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.3}Implementation Results}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.4}APSP Matrix Multiplication}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A graph of the runtime APSP Matrix Multiplication against the number of vertices.}}{62}}
\newlabel{fig:apspruntime}{{4.1}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Output Parallel Bellman-Ford}{62}}
